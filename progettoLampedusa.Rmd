---
title: "ProgettoLampedusa"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
  word_document: default
author: "Lampedusa_Group"
editor_options: 
  markdown: 
    wrap: 72
---

##I Rimbalzi Nel Basket


| Con questo report ci siamo interessati allo studio di un modello lienare predittivo per le vittorie, basato sul numero di rimbalzi annuali effettuati      dalle squadre nel campionato NBA.
| vengono prese in considerazione solo le squadre con almeno 82 partite disputate, con riferimento agli anni dal 1976 al 2011.

| La previsione del modello lineare e' basata sull'uso di coefficienti ideati per essere il piu possibile significativi e conforme alla nostra idea di       incidenza sulle vittorie di certe variabili riespetto ad altre.

| I dettagli su come eseguire il programma e utilizzare la funzione predittiva si trovano direttamente di seguito, assieme al processo di realizzazione e    verifica del modello stesso.

```{r setup, include=TRUE, warning=FALSE, message=FALSE, error=FALSE}

library(here)
library(corrplot)
library(nortest)
library(lmtest)
library(car)
library(heatmaply)
library(ggheatmap)
library(ggplot2)
library(viridis)

knitr::opts_knit$set(root.dir = here("0_Materiale"))
```

# Progetto numero 5

NBA moderna (1976-2011): VARIABILE DIPENDENTE: numero di vittorie in stagione COVARIATE: tutte le altre (o uno specifico insieme di queste, in base all'obiettivo di analisi) Considerare solo le squadre che hanno giocato 82 partite (dataset\$games==82)

## INIZIALIZZAZIONE DATI E GRAFICI DATI
```{r}
#SCREMATURA DEL SET

dataset <- read.delim("basketball_teams.txt") # andiamo a leggere il database fornito
FIRST <- 1976 # primo anno del range da considerare per lo studio
LAST <- 2011 # ultimo anno del range da considerare per lo studio

df <- dataset [dataset$lgID=="NBA" & dataset$year >= FIRST & dataset$year <= LAST & dataset$games==82,]

dataset$lgID <- as.factor(dataset$lgID) # perchè mi permettono di poter generare variabili dummy
summary(df)

```

```{r}
#STUDIO DATI RELATIVI AI RIMBALZI

# o_oreb = Rimbalzi ottenuti in attacco
# o_dreb = Rimbalzi subiti in attacco
# o_reb  = totale rimbalzi in attacco
# d_oreb = Rimbalzi subiti in difesa
# d_dreb = Rimbalzi ottenuti in difesa
# d_reb  = totale rimbalzi in difesa

df_reb <- subset(df, select = c("o_oreb", "o_dreb", "o_reb", "d_oreb", "d_dreb", "d_reb"))
  
for (variables in 1:(dim(df_reb)[2])){
  thisvar = df_reb[,variables]
  d <- density(thisvar)
  plot(d, main = names(df_reb[variables]),xlab="")
  polygon(d, col="cyan", border="blue")
  title("Density plots for all Model Variables", line = -19, outer = TRUE)}

#verifica della presenza zeri nel dataset
print(paste("Percentage non-zero o_oreb: ",round(length(which(df_reb$o_oreb>0)) /dim(df_reb)[1]*100,2)))
print(paste("Percentage non-zero o_dreb: ",round(length(which(df_reb$o_dreb>0)) /dim(df_reb)[1]*100,2)))
print(paste("Percentage non-zero o_reb: ",round(length(which(df_reb$o_reb>0)) /dim(df_reb)[1]*100,2)))
print(paste("Percentage non-zero d_oreb: ",round(length(which(df_reb$d_oreb>0)) /dim(df_reb)[1]*100,2)))
print(paste("Percentage non-zero d_dreb: ",round(length(which(df_reb$d_dreb>0)) /dim(df_reb)[1]*100,2)))
print(paste("Percentage non-zero d_reb: ",round(length(which(df_reb$d_reb>0)) /dim(df_reb)[1]*100,2)))



```

```{r}
#STUDIO SULLE VITTORIE

M <- cor(as.matrix(df[, c(11:25, 54)])) # correlation matrix
corrplot(M, method="color", outline = TRUE,type="lower",order = "hclust",
        tl.col="black", tl.srt=45, diag=FALSE,tl.cex = 1,mar=c(0,0,3,0),
        title="Correlation Matrix between Predictor and Outcome variables")






#ISTOGRAMMA
# Crea un istogramma di base
hist(df$won, col = "skyblue", border = "white", main = "Distribuzione delle Vittorie", xlab = "Numero di Vittorie", ylab = "Frequenza")

# Aggiungi una griglia di sfondo
grid()

# Aggiungi una linea di riferimento
abline(v = mean(df$won), col = "red", lwd = 2)

# Aggiungi una legenda
legend("topright", legend = c("Media"), col = c("red"), lwd = 2)







# PLOT DENSITA

density_plot <- density(df$won)

# Plot di base
plot(density_plot, main = "Distribuzione di Densità delle Vittorie", col = "skyblue", lwd = 2, ylim = c(0, 0.07), xlim = c(0, max(df$won)))

# Aggiungi titoli ed etichette degli assi
title(main = "Distribuzione di Densità delle Vittorie", xlab = "Numero di Vittorie", ylab = "Densità")

# Aggiungi una griglia di sfondo
grid()

# Aggiungi una linea di riferimento per la media
abline(v = mean(df$won), col = "red", lwd = 2)

# Aggiungi una legenda
legend("topright", legend = c("Media"), col = c("red"), lwd = 2)







#CORRPLOT 

# Calcola la matrice di correlazione
M <- cor(as.matrix(df[, c(11:25, 54)]))

# Crea il grafico della matrice di correlazione
corrplot(M, method = "color", outline = TRUE, type = "lower", order = "hclust",
         tl.col = "black", tl.srt = 45, diag = FALSE, tl.cex = 0.7, mar = c(0, 0, 3, 0),
         title = "Correlation Matrix between Predictor and Outcome variables",
         addCoef.col = "black", number.cex = 0.5, tl.offset = 0.5)

# Aggiungi una legenda
legend("bottomleft", legend = c("Positive Correlation", "Negative Correlation"), fill = c("white", "black"), border = NA, bty = "n", title = "Correlation")







#BOXPLOT

# Crea un boxplot con aggiunta di titoli ed etichette degli assi
boxplot(df$won ~ df$tmID, col = "skyblue", main = "Boxplot delle Vittorie per tmID", col.main = "black", font.main = 4,
        xlab = "tmID",col.lab = "black", font.lab = 2, ylab = "Numero di Vittorie", col.lab = "black", font.lab = 2)

# Aggiungi una griglia di sfondo
grid()

# Personalizza i colori delle scatole
boxplot(df$won ~ df$tmID, col = c("skyblue", "lightgreen"), add = TRUE, border = "black")

# Aggiungi una legenda
legend("topright", legend = unique(df$tmID), fill = c("skyblue", "lightgreen"), title = "tmID")

# Inclina le etichette sull'asse x
par(las = 2, cex.axis = 0.8)




df$reb <- df$o_reb + df$d_reb

```

## TESTS DI VERIFICA

### TEST ANDERSON-DARLING

```{r}
ad.test(df$reb)
```

Con un livello di significatività ($\alpha$) di 0.01 e un p-value molto piccolo (3.1e-09) ottenuto dal test di normalità di Anderson-Darling per i dati della variabile df\$reb, puoi concludere che hai sufficiente evidenza statistica per respingere lipotesi nulla che i dati seguono una distribuzione normale.Con il tuo livello di significatività del 0.01 e il p-value molto piccolo (3.1e-09), il p-value è inferiore al livello di significatività, quindi respingeresti lipotesi nulla. Questo suggerisce che i dati nella variabile df\$reb non seguono una distribuzione normale al livello di significatività del 0.01. In termini più pratici, hai abbastanza evidenza statistica per concludere che la variabile df\$reb non segue una distribuzione normale basandoti sui risultati del test di Anderson-Darling.

### TEST KOLMOGOROV SMIRNOV

```{r}
ks.test(df$reb, "pnorm")
```

Il risultato che hai ottenuto riguarda il test di Kolmogorov-Smirnov a campione singolo sui dati contenuti nella variabile df\$reb. Il test KS confronta la distribuzione empirica dei tuoi dati con una distribuzione teorica (spesso una distribuzione uniforme). In breve, il risultato suggerisce che i tuoi dati non seguono la distribuzione teorica presunta, e cè un elevata probabilità che la differenza osservata sia statisticamente significativa.

### TEST SHAPIRO WILK

```{r}
sf.test(df$reb)
```

In sintesi, il risultato del test di Shapiro-Francia indica che i tuoi dati nella variabile df\$reb non seguono una distribuzione normale. Questo è supportato dal valore basso del p-value, il quale suggerisce che la differenza tra la distribuzione dei tuoi dati e una distribuzione normale è statisticamente significativa.

## INIZIALIZZAZIONE MODELLO DI REGRESSIONE LINEARE

### L'IMPORTANZA DEI RIMBALZI

$\text{Formula1} = \frac{\text{Rimbalzi offensivi in attacco}}{\text{Tiri sbagliati su azione}}$ Rappresenta la capacità della squadra di ripossesso della palla dopo un tiro che non va a canestro e colpisce il tabellone.

$\text{Formula2} = \frac{\text{Rimbalzi difensivi in difesa presi}}{\text{Tiri sbagliati su azione degli avversari}}$ Rappresenta la capacità della squadra di impossessarsi della palla dopo un tiro sbagliato della squadra avversaria che colpisce il tabellone, che troviamo un buon stimatore della capacità di contropiede della squadra.

$\text{Formula3} = \frac{\text{Palle riprese in attacco} + 1.5 \times \text{Palle riprese in difesa}}{\text{Palle perse in attacco} + 2 \times \text{Rimbalzi subiti in difesa}}$ Rappresenta il rapporto tra le palle riprese nei rimbalzi (sia offensivi che difensivi) rispetto alle palle perse nei rimbalzi (sia offensivi che difensivi). I coefficienti sono stati scelti in base a ciò che riteniamo più importante in una partita, ossia la difesa del proprio canestro.

$\text{Formula4} = (\text{Palle riprese in attacco - Palle perse in attacco}) + 1.5*(\text{Palle riprese in difesa - Palle perse in difesa})$ Cresce all'aumentare dei rimbalzi ottenuti e diminuisce all'aumentare dei rimbalzi subiti, considerando anche un coefficiente che da particolare importanza alla difesa.

$\text{Formula5} = \frac{(\frac{\text{Rimbalzi subiti in difesa}}{\text{Palle perse in difesa}})}{(\frac{\text{Rimbalzi subiti in attacco}}{\text{Palle perse in attacco}})}$ Mostra quanto siano influenti i rimbalzi nel rapporto tra le palle perse dalla squadra e le palle perse dagli avversari.

```{r}
# o_oreb = Rimbalzi ottenuti in attacco
# o_dreb = Rimbalzi subiti in attacco
# o_reb  = totale rimbalzi in attacco
# d_oreb = Rimbalzi subiti in difesa
# d_dreb = Rimbalzi ottenuti in difesa
# d_reb  = totale rimbalzi in difesa

df$f1 <- (df$o_oreb)/(df$o_fga-df$o_fgm)
df$f2 <- (df$d_dreb)/(df$d_fga-df$d_fgm)
df$f3 <- (df$o_oreb + 1.5 * df$d_dreb)/(df$o_dreb + 2 * df$d_oreb)
df$f4 <- (df$o_oreb - df$o_dreb) + 1.5 * (df$d_dreb - df$d_oreb)
df$f5 <- (df$d_oreb / df$d_to) / (df$o_dreb / df$o_to)


linMod <- lm(won ~ f1 + f2 + f3 + f4 + f5, data = df)
summary (linMod)

plot(linMod)

```

## INIZIALIZZAZIONE MODELLO DI REGRESSIONE LINEARE NORMALIZZATO

```{r}
# In un chunk diverso per minimizzare cpu-time

# Normalizziamo le covariate
df$f1_z <- scale(df$f1)
df$f2_z <- scale(df$f2)
df$f3_z <- scale(df$f3)
df$f4_z <- scale(df$f4)
df$f5_z <- scale(df$f5)

linModNormalized <- lm(won ~ f1_z + f2_z + f3_z + f4_z + f5_z, data = df)

summary(linModNormalized)
plot(linModNormalized)

```

## TEST SUL MODELLO DI REGRESSIONE LINEARE

### TEST BREUSCH-PAGAN (Test di omoschedasticità)

```{r}

# TEST SUL MODELLO DI REGRESSIONE LINEARE

#1 Summary
summary (linModNormalized)

#2 R-quadrato e R-quadrato Adattato
summary_linModNormalized <- summary(linModNormalized)
r_squared <- summary_linModNormalized$r.squared
cat("R-squared:", r_squared, "\n")


n <- length(df$o_oreb)
k <- length(linModNormalized$coefficients) - 1
adjusted_r_squared <- 1 - ((1 - r_squared) * (n - 1) / (n - k - 1))
cat("Adjusted R-squared:", adjusted_r_squared, "\n")

#2 test Shapiro per valutare la normalita' dei residui
shapiro.test(residuals(linModNormalized))

#3 test di omoschedasticita' (Breusch-Pagan test) --> risultato suggerisce omoschedasiticita'
bptest(linModNormalized)

#4 test di multicollinearita'
car::vif(linModNormalized)

```

Possiamo ora verificare la presenza di outlayers, ovvero dati anomali o fuori norma che possono influenzare in modo molto significativo il modello rispetto al resto dei dati presi in cosiderazione.
Provvediamo quindi a rimuoverli e a ricreare il modello.
```{r, include=TRUE}
influencePlot(linModNormalized,5)

#rimuovo gli outlayers e ricreo il modello
df_1 <- df[-which(row.names(df) %in% c(822, 957)),]

df_1$f1 <- (df_1$o_oreb)/(df_1$o_fga-df_1$o_fgm)
df_1$f2 <- (df_1$d_dreb)/(df_1$d_fga-df_1$d_fgm)
df_1$f3 <- (df_1$o_oreb + 1.5 * df_1$d_dreb)/(df_1$o_dreb + 2 * df_1$d_oreb)
df_1$f4 <- (df_1$o_oreb - df_1$o_dreb) + 1.5 * (df_1$d_dreb - df_1$d_oreb)
df_1$f5 <- (df_1$d_oreb / df_1$d_to) / (df_1$o_dreb / df_1$o_to)

df_1$f1_z <- scale(df_1$f1)
df_1$f2_z <- scale(df_1$f2)
df_1$f3_z <- scale(df_1$f3)
df_1$f4_z <- scale(df_1$f4)
df_1$f5_z <- scale(df_1$f5)

linModNormalized_1 <- lm(won ~ f1_z + f2_z + f3_z + f4_z + f5_z, data = df_1)

influencePlot(linModNormalized_1,5)

summary(linModNormalized)
summary(linModNormalized_1)

```
Nel complesso, entrambi i modelli sembrano essere buoni modelli di regressione, Entrambi i modelli spiegano circa l'84% della varianza nella variabile dipendente "won".


```{r}
# Divisione in Test e Train per evitare che il modello fitti troppo bene sui nostri dati
sample <- sample(c(TRUE, FALSE), nrow(df), replace=TRUE, prob=c(0.7, 0.3))
train  <- df[sample, ]
test   <- df[!sample, ]


```


Il risultato suggerisce omoschedasiticita'

```{r}

# SANDBOX TESTING
# QUI SI TESTA TUTTO CIÒ CHE POI VERRÀ AGGGIUNTO NEL DOCUMENTO SOPRA

values <- aggregate(cbind(o_oreb, o_dreb, d_oreb, d_dreb, o_reb, d_reb) ~ tmID, data = df, FUN = sum)

# temp <- hist (temp, col = 'steelblue', main = 'caccaculo', xlab = 'balls')

### SANDBOX


teams <- c()

df$reb <- c(df$o_reb + df$d_reb)

summary(df$reb)

for (team in df$tmID)
{
  teams <- unique(c(teams, team))
}

teams <- sort(teams)

values <- aggregate(cbind(o_oreb, o_dreb, d_oreb, d_dreb, o_reb, d_reb) ~ tmID, data = df, FUN = sum)

x <- seq(-4, 4, by = 0.01)
y <- dnorm(x, mean = 0, sd = 1)
plot (x,y, type="l")
normal <- data.frame(x, y)


df$reb <- df$o_reb + df$d_reb




#ISTOGRAMMA 

hist(df$reb, 
     main = "Istogramma dei Rimbalzi",
     xlab = "Rimbalzi",
     ylab = "Frequenza",
     col = "lightblue",      # Colore delle barre
     border = "black",       # Colore del bordo delle barre
     breaks = 20,            # Numero di intervalli (personalizzabile)
     xlim = c(5800, max(df$reb)),  # Imposta i limiti dell'asse x
     ylim = c(0, 150)         # Limite dell'asse y (personalizzabile)
)

# Aggiungi una griglia di sfondo
grid()

# Aggiungi una legenda
legend("topright", legend = c("Rimbalzi"), fill = "lightblue")

# Aggiungi una linea media se necessario
abline(v = mean(df$reb), col = "red", lwd = 2, lty = 2)




#PLOT DI DENSITA

plot(density(df$reb), 
     main = "Densità dei Rimbalzi",
     xlab = "Rimbalzi",
     col = "blue",         # Colore della curva di densità
     lwd = 2               # Spessore della curva di densità
)

# Aggiungi una griglia di sfondo
grid()

# Aggiungi etichette per gli assi
axis(1, col = "darkgray")  # Asse x
axis(2, col = "darkgray")  # Asse y

# Aggiungi una legenda
legend("topright", legend = c("Densità dei Rimbalzi"), col = "blue", lwd = 2)

# Aggiungi una linea per la media se necessario
abline(v = mean(df$reb), col = "red", lwd = 2, lty = 2)




#PLOT DI DENSITA CON SOVRAPPOSIZIONE DI UNA NORMALE IDEALE

# Crea il grafico di densità
plot(density(df$reb), 
     main = "Densità dei Rimbalzi", cex.main = 2, font.main = 2,
     xlab = "Rimbalzi", cex.lab = 1.2, font.lab = 1.6,
     col = "blue",         # Colore della curva di densità osservata
     lwd = 2               # Spessore della curva di densità osservata
)

# Aggiungi una griglia di sfondo
grid()

# Aggiungi etichette per gli assi
axis(1, col = "darkgray")  # Asse x
axis(2, col = "darkgray")  # Asse y

# Aggiungi una legenda
legend("topright", legend = c("Densità dei Rimbalzi"), col = "blue", lwd = 2)

# Aggiungi una linea per la media se necessario
abline(v = mean(df$reb), col = "red", lwd = 2, lty = 2)

# Aggiungi una curva di densità normale teorica
mu <- mean(df$reb)
sigma <- sd(df$reb)
x <- seq(min(df$reb), max(df$reb), length = 100)
lines(x, dnorm(x, mean = mu, sd = sigma), col = "green", lwd = 2)

# Aggiungi una legenda per la curva normale
legend("topright", legend = c("Densità dei Rimbalzi", "Densità Normale"), col = c("blue", "green"), lwd = 2)





# INSTOGRAMMA CON SOVRAPPOSIZIONE DI PLOT DI DENSITA DEL DATASET E IL PLOT DI UNA NORMALE IDEALE

hist(df$reb, 
     col = 'skyblue',      # Colore delle barre
     prob = TRUE, 
     main = "Istogramma con Densità", 
     xlab = "Rimbalzi", 
     ylab = "Densità",
     border = "white"      # Colore del bordo delle barre
)

# Aggiungi una linea di densità per i dati del dataframe
lines(density(df$reb), col = "red", lwd = 2)

# Aggiungi una curva di densità normale teorica
mu <- mean(df$reb)
sigma <- sd(df$reb)
x <- seq(min(df$reb), max(df$reb), length = 100)
lines(x, dnorm(x, mean = mu, sd = sigma), col = "darkgreen", lwd = 2)

# Aggiungi una legenda
legend("topright", legend = c("Densità Osservata", "Densità Normale", "Rimbalzi"), col = c("red", "darkgreen", "lightblue"), lwd = 2)

# Aggiungi una griglia di sfondo
grid(lty = 3, col = "lightgray")

# Aggiungi una cornice intorno all'istogramma
box()

# Personalizza l'aspetto del titolo e delle etichette
title(main = "Istogramma con Densità", cex.main = 1.5)
title(xlab = "Rimbalzi", cex.lab = 1.2)
title(ylab = "Densità", cex.lab = 1.2)





# CORRPLOT

# Seleziona le colonne del DataFrame
data_subset <- df[, c("reb", "o_reb", "d_reb")]

# Calcola la matrice di correlazione
cor_matrix <- cor(data_subset)

# Crea un corrplot con colori accattivanti
corrplot(cor_matrix, 
         method = "circle", 
         order = "hclust", 
         tl.cex = 0.8, 
         tl.col = "black",
         addrect = 3,  # Aggiungi un rettangolo intorno al blocco di correlazione
         rect.col = "lightblue",  # Colore del rettangolo
         bg = "white",  # Colore di sfondo
         cl.ratio = 0.2,  # Spazio tra celle
         col = colorRampPalette(c("#4575b4", "#91bfdb", "#e0f3f8", "#fee08b", "#d73027"))(100),  # Scala di colori personalizzata
         addCoef.col = "black",  # Colore del testo dei coefficienti
         number.cex = 0.7,  # Dimensione del testo dei coefficienti
         mar = c(0,0,2,0)  # Margine per evitare il taglio del titolo
)





#BOXPLOT (1° Versione)

boxplot(df$won ~ df$reb, las = 2, col = "skyblue", main = "Distribuzione delle Vittorie in base ai Rimbalzi", cex.main = 1.5, font.main = 2, xlab = "Rimbalzi", ylab = "Vittorie", border = "black", notch = FALSE)

# Aggiungi una griglia di sfondo più sottile
grid(lty = 3, col = "lightgray")

# Aggiungi una legenda
legend("topright", legend = c("Vittorie"), col = "skyblue", lwd = 2, cex = 1.2)

# Aggiungi una linea per la mediana
abline(h = median(df$won), col = "red", lty = 2, lwd = 2)

# Aggiungi un'etichetta per la mediana
text(max(df$reb) + 0.2, median(df$won), "Mediana", col = "red", pos = 2, offset = 0.5, cex = 1.2)





#BOXPLOT (2° Versione)

# Creare categorie per i rimbalzi
df$reb_categories <- cut(df$reb, breaks = c(-Inf, 5, 10, 15, Inf), labels = c("0-5", "6-10", "11-15", "16+"))

# Convertire df$won in numeri
df$won <- as.numeric(as.character(df$won))

# Creare un boxplot con ggplot2
ggplot(df, aes(x = reb_categories, y = won, fill = reb_categories)) +
  geom_boxplot(color = "black", notch = FALSE) +
  labs(title = "Distribuzione delle Vittorie in base ai Rimbalzi",
       x = "Rimbalzi",
       y = "Vittorie") +
  theme_minimal()






#TEST ANDERSON-DARLING
ad.test(df$reb)

#Con un livello di significatività (α) di 0.01 e un p-value molto piccolo (3.1e-09) ottenuto dal test di normalità di Anderson-Darling per i dati della variabile df$reb, puoi concludere che hai sufficiente evidenza statistica per respingere lipotesi nulla che i dati seguono una distribuzione normale.Con il tuo livello di significatività del 0.01 e il p-value molto piccolo (3.1e-09), il p-value è inferiore al livello di significatività, quindi respingeresti lipotesi nulla. Questo suggerisce che i dati nella variabile df$reb non seguono una distribuzione normale al livello di significatività del 0.01. In termini più pratici, hai abbastanza evidenza statistica per concludere che la variabile df$reb non segue una distribuzione normale basandoti sui risultati del test di Anderson-Darling.


#TEST KOLMOGOROV SMIRNOV

ks.test(df$reb, "pnorm")

# Il risultato che hai ottenuto riguarda il test di Kolmogorov-Smirnov a campione singolo sui dati contenuti nella variabile df$reb. Il test KS confronta la distribuzione empirica dei tuoi dati con una distribuzione teorica (spesso una distribuzione uniforme). In breve, il risultato suggerisce che i tuoi dati non seguono la distribuzione teorica presunta, e cè un elevata probabilità che la differenza osservata sia statisticamente significativa.


#TEST SHAPIRO WILK

sf.test(df$reb)

#In sintesi, il risultato del test di Shapiro-Francia indica che i tuoi dati nella variabile df$reb non seguono una distribuzione normale. Questo è supportato dal valore basso del p-value, il quale suggerisce che la differenza tra la distribuzione dei tuoi dati e una distribuzione normale è statisticamente significativa.


barplot(df$reb, col = c("#1b98e0", "#353436"))

```

```{r}

heatmap_df <- subset(values, select = -c(o_oreb, o_dreb, d_oreb, d_dreb))
rownames(heatmap_df) <- heatmap_df$tmID
heatmap_df <- heatmap_df[,-1]

# Fare URIEL ---> da SISTEMARE
heatmaply(
  percentize(heatmap_df),
  colors = viridis(n = 256,  option = "magma"),
  k_col = 2,
  k_row = 4,
)
# pie()
barplot(df$reb, col = c("#1b98e0", "#353436"))
legend("topright", legend = c("Group 1", "Group 2"), fill = c("#1b98e0", "#353436"))

```
