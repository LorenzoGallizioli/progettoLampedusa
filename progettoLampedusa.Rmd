---
title: "ProgettoLampedusa"
date: "2023-12-07"
output:
  pdf_document: default
  html_document: default
  word_document: default
author: "Lampedusa_Group"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=TRUE}

knitr::opts_chunk$set(echo = TRUE)

library(here)
library(corrplot)
library(nortest)
library(lmtest)
library(car)

knitr::opts_knit$set(root.dir = here("0_Materiale"))
```

# Progetto numero 5
NBA moderna (1976-2011): VARIABILE DIPENDENTE: numero di vittorie in stagione
COVARIATE: tutte le altre (o uno specifico insieme di queste, in base all'obiettivo di analisi)
Considerare solo le squadre che hanno giocato 82 partite (dataset$games==82)

## INIZIALIZZAZIONE DATI E GRAFICI DATI
```{r}
dataset <- read.delim("basketball_teams.txt") # andiamo a leggere il database fornito
FIRST <- 1976 # primo anno del range da considerare per lo studio
LAST <- 2011 # ultimo anno del range da considerare per lo studio

df <- dataset [dataset$lgID=="NBA" & dataset$year >= FIRST & dataset$year <= LAST & dataset$games==82,]

dataset$lgID <- as.factor(dataset$lgID) # perchè mi permettono di poter generare variabili dummy
summary(df)
hist(df$won)
plot(density(df$won))

M <- cor(as.matrix(df[, c(11:25, 54)])) # correlation matrix
corrplot(M, method="color", outline = TRUE,type="lower",order = "hclust",
        tl.col="black", tl.srt=45, diag=FALSE,tl.cex = 1,mar=c(0,0,3,0),
        title="Correlation Matrix between Predictor and Outcome variables")

boxplot(df$won ~ df$tmID, las=2)

df$reb <- df$o_reb + df$d_reb

```

## TESTS DI VERIFICA

### TEST ANDERSON-DARLING
```{r}
ad.test(df$reb)
```
Con un livello di significatività ($\alpha$) di 0.01 e un p-value molto piccolo (3.1e-09) ottenuto dal test di normalità di Anderson-Darling per i dati della variabile df\$reb, puoi concludere che hai sufficiente evidenza statistica per respingere lipotesi nulla che i dati seguono una distribuzione normale.Con il tuo livello di significatività del 0.01 e il p-value molto piccolo (3.1e-09), il p-value è inferiore al livello di significatività, quindi respingeresti lipotesi nulla. Questo suggerisce che i dati nella variabile df\$reb non seguono una distribuzione normale al livello di significatività del 0.01. In termini più pratici, hai abbastanza evidenza statistica per concludere che la variabile df\$reb non segue una distribuzione normale basandoti sui risultati del test di Anderson-Darling.

### TEST KOLMOGOROV SMIRNOV
```{r}
ks.test(df$reb, "pnorm")
```
Il risultato che hai ottenuto riguarda il test di Kolmogorov-Smirnov a campione singolo sui dati contenuti nella variabile df$reb. Il test KS confronta la distribuzione empirica dei tuoi dati con una distribuzione teorica (spesso una distribuzione uniforme). In breve, il risultato suggerisce che i tuoi dati non seguono la distribuzione teorica presunta, e cè un elevata probabilità che la differenza osservata sia statisticamente significativa.

### TEST SHAPIRO WILK
```{r}
sf.test(df$reb)
```
In sintesi, il risultato del test di Shapiro-Francia indica che i tuoi dati nella variabile df$reb non seguono una distribuzione normale. Questo è supportato dal valore basso del p-value, il quale suggerisce che la differenza tra la distribuzione dei tuoi dati e una distribuzione normale è statisticamente significativa.


## INIZIALIZZAZIONE MODELLO DI REGRESSIONE LINEARE

### L'IMPORTANZA DEI RIMBALZI

$\text{Formula1} = \frac{\text{Rimbalzi offensivi in attacco}}{\text{Tiri sbagliati su azione}}$
Rappresenta la capacità della squadra di ripossesso della palla dopo un tiro che non va a canestro e colpisce il tabellone.

$\text{Formula2} = \frac{\text{Rimbalzi difensivi in difesa presi}}{\text{Tiri sbagliati su azione degli avversari}}$
Rappresenta la capacità della squadra di impossessarsi della palla dopo un tiro sbagliato della squadra avversaria che colpisce il tabellone, che troviamo un buon stimatore della capacità di contropiede della squadra.

$\text{Formula3} = \frac{\text{Palle riprese in attacco} + 1.5 \times \text{Palle riprese in difesa}}{\text{Palle perse in attacco} + 2 \times \text{Rimbalzi subiti in difesa}}$
Rappresenta il rapporto tra le palle riprese nei rimbalzi (sia offensivi che difensivi) rispetto alle palle perse nei rimbalzi (sia offensivi che difensivi). I coefficienti sono stati scelti in base a ciò che riteniamo più importante in una partita, ossia la difesa del proprio canestro.

$\text{Formula4} = (\text{Palle riprese in attacco - Palle perse in attacco}) + 1.5*(\text{Palle riprese in difesa - Palle perse in difesa})$
Cresce all'aumentare dei rimbalzi ottenuti e diminuisce all'aumentare dei rimbalzi subiti, considerando anche un coefficiente che da particolare importanza alla difesa.

$\text{Formula5} = \frac{(\frac{\text{Rimbalzi subiti in difesa}}{\text{Palle perse in difesa}})}{(\frac{\text{Rimbalzi subiti in attacco}}{\text{Palle perse in attacco}})}$
Mostra quanto siano influenti i rimbalzi nel rapporto tra le palle perse dalla squadra e le palle perse dagli avversari.

```{r}
# o_oreb = Rimbalzi ottenuti in attacco
# o_dreb = Rimbalzi subiti in attacco
# o_reb  = totale rimbalzi in attacco
# d_oreb = Rimbalzi subiti in difesa
# d_dreb = Rimbalzi ottenuti in difesa
# d_reb  = totale rimbalzi in difesa

df$f1 <- (df$o_oreb)/(df$o_fga-df$o_fgm)
df$f2 <- (df$d_dreb)/(df$d_fga-df$d_fgm)
df$f3 <- (df$o_oreb + 1.5 * df$d_dreb)/(df$o_dreb + 2 * df$d_oreb)
df$f4 <- (df$o_oreb - df$o_dreb) + 1.5 * (df$d_dreb - df$d_oreb)
df$f5 <- (df$d_oreb / df$d_to) / (df$o_dreb / df$o_to)

<<<<<<< HEAD
=======

>>>>>>> fceeb704869ea08a9ddfef50e705ab6ad6e4df3f
linMod <- lm(won ~ f1 + f2 + f3 + f4 + f5, data = df)
summary (linMod)

plot(linMod)
<<<<<<< HEAD
=======

>>>>>>> fceeb704869ea08a9ddfef50e705ab6ad6e4df3f
```

## INIZIALIZZAZIONE MODELLO DI REGRESSIONE LINEARE NORMALIZZATO

```{r}
# In un chunk diverso per minimizzare cpu-time

# Normalizziamo le covariate
df$f1_z <- scale(df$f1)
df$f2_z <- scale(df$f2)
df$f3_z <- scale(df$f3)
df$f4_z <- scale(df$f4)
df$f5_z <- scale(df$f5)

linModNormalized <- lm(won ~ f1_z + f2_z + f3_z + f4_z + f5_z, data = df)

```

## TEST SUL MODELLO DI REGRESSIONE LINEARE

### TEST BREUSCH-PAGAN (Test di omoschedasticità)
```{r}

# TEST SUL MODELLO DI REGRESSIONE LINEARE

#1 Summary
summary (linModNormalized)

#2 R-quadrato e R-quadrato Adattato
summary_linModNormalized <- summary(linModNormalized)
r_squared <- summary_linModNormalized$r.squared
cat("R-squared:", r_squared, "\n")


n <- length(df$o_oreb)
k <- length(linModNormalized$coefficients) - 1
adjusted_r_squared <- 1 - ((1 - r_squared) * (n - 1) / (n - k - 1))
cat("Adjusted R-squared:", adjusted_r_squared, "\n")

#2 test Shapiro per valutare la normalita' dei residui
shapiro.test(residuals(linModNormalized))

#3 test di omoschedasticita'
bptest(linModNormalized)

#4 test di multicollinearita'
car::vif(linModNormalized)



# Test di homoschedasticita' (Breusch-Pagan test) --> risultato suggerisce omoschedasiticita'

lmtest::bptest(linModNormalized)
# Divisione in Test e Train per evitare che il modello fitti troppo bene sui nostri dati
sample <- sample(c(TRUE, FALSE), nrow(df), replace=TRUE, prob=c(0.7, 0.3))
train  <- df[sample, ]
test   <- df[!sample, ]

# m <- lm(won ~ o_canestriSuTotali_z + d_canestriSuTotali_z + d_stoppateSuTiri_z + o_rimbTiriSbagliati_z + d_rimbDef_z, data = # train)
# summary(m)
```
Il risultato suggerisce omoschedasiticita'

<<<<<<< HEAD

```{r}

values <- aggregate(cbind(o_oreb, o_dreb, d_oreb, d_dreb, o_reb, d_reb) ~ tmID, data = df, FUN = sum)

# temp <- hist (temp, col = 'steelblue', main = 'caccaculo', xlab = 'balls')
=======
### SANDBOX

```{r}

teams <- c()

df$reb <- c(df$o_reb + df$d_reb)

summary(df$reb)

for (team in df$tmID)
{
  teams <- unique(c(teams, team))
}

teams <- sort(teams)

values <- aggregate(cbind(o_oreb, o_dreb, d_oreb, d_dreb, o_reb, d_reb) ~ tmID, data = df, FUN = sum)

x <- seq(-4, 4, by = 0.01)
y <- dnorm(x, mean = 0, sd = 1)
plot (x,y, type="l")
normal <- data.frame(x, y)


df$reb <- df$o_reb + df$d_reb
hist (df$reb, col = 'steelblue', main = 'caccaculo', xlab = 'balls')

>>>>>>> fceeb704869ea08a9ddfef50e705ab6ad6e4df3f

plot(density(df$reb))
lines(density(normal), col = "green")


hist(df$reb, col = 'steelblue', prob = TRUE, main = "Istogramma con Densità")

# Aggiungere la linea di densità per i dati del dataframe
lines(density(df$reb), col = "red", lwd = 2)

# Aggiungere la linea di densità per la distribuzione normale
lines(density(normal), col = "green", lwd = 2)

# Aggiungere una legenda
legend("topright", legend = c("Dati", "Densità Normale"), col = c("red", "green"), lwd = 2)


df_reb <- data.frame (df$reb, df$o_reb, df$d_reb)
M <- cor(df_reb) # correlation matrix
corrplot(M, method="color", outline = TRUE,type="lower",order = "hclust",
         tl.col="black", tl.srt=45, diag=FALSE,tl.cex = 1,mar=c(0,0,3,0),
         title="culocacca")


#TEST ANDERSON-DARLING

install.packages('nortest')
library(nortest)

ad.test(df$reb)

#Con un livello di significatività (α) di 0.01 e un p-value molto piccolo (3.1e-09) ottenuto dal test di normalità di Anderson-Darling per i dati della variabile df$reb, puoi concludere che hai sufficiente evidenza statistica per respingere lipotesi nulla che i dati seguono una distribuzione normale.Con il tuo livello di significatività del 0.01 e il p-value molto piccolo (3.1e-09), il p-value è inferiore al livello di significatività, quindi respingeresti lipotesi nulla. Questo suggerisce che i dati nella variabile df$reb non seguono una distribuzione normale al livello di significatività del 0.01. In termini più pratici, hai abbastanza evidenza statistica per concludere che la variabile df$reb non segue una distribuzione normale basandoti sui risultati del test di Anderson-Darling.


#TEST KOLMOGOROV SMIRNOV

ks.test(df$reb, "pnorm")

# Il risultato che hai ottenuto riguarda il test di Kolmogorov-Smirnov a campione singolo sui dati contenuti nella variabile df$reb. Il test KS confronta la distribuzione empirica dei tuoi dati con una distribuzione teorica (spesso una distribuzione uniforme). In breve, il risultato suggerisce che i tuoi dati non seguono la distribuzione teorica presunta, e cè un elevata probabilità che la differenza osservata sia statisticamente significativa.


#TEST SHAPIRO WILK

sf.test(df$reb)

#In sintesi, il risultato del test di Shapiro-Francia indica che i tuoi dati nella variabile df$reb non seguono una distribuzione normale. Questo è supportato dal valore basso del p-value, il quale suggerisce che la differenza tra la distribuzione dei tuoi dati e una distribuzione normale è statisticamente significativa.

<<<<<<< HEAD
=======

>>>>>>> fceeb704869ea08a9ddfef50e705ab6ad6e4df3f
barplot(df$reb, col = c("#1b98e0", "#353436"))

legend("topright", legend = c("Group 1", "Group 2"), fill = c("#1b98e0", "#353436"))



# Fare URIEL ---> da SISTEMARE
heatmap(cbind(df$d_reb, df$o_reb))
pie()
boxplot(df$won ~ df$reb, las=2)

```
