---
title: "ProgettoLampedusa"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
  word_document: default
author: "Lampedusa_Group"
editor_options: 
  markdown: 
    wrap: 72
---

##I Rimbalzi Nel Basket

Con questo report ci siamo interessati allo studio di un modello lienare predittivo per le vittorie, basato sul numero di rimbalzi annuali effettuati dalle squadre nel campionato NBA.
vengono prese in considerazione solo le squadre con almeno 82 partite disputate, con riferimento agli anni dal 1976 al 2011.

La previsione del modello lineare e' basata sull'uso di coefficienti ideati per essere il piu possibile significativi e conforme alla nostra idea di incidenza sulle vittorie di certe variabili riespetto ad altre.

I dettagli su come eseguire il programma e utilizzare la funzione predittiva si trovano direttamente di seguito, assieme al processo di realizzazione e verifica del modello stesso.


```{r setup, include=TRUE, warning=FALSE, message=FALSE, error=FALSE}

library(here)
library(corrplot)
library(nortest)
library(lmtest)
library(car)
library(plotly)
library(heatmaply)
library(ggheatmap)
library(ggplot2)
library(viridis)
library(glmnet)
library(highcharter)
library(reshape2)
library(RColorBrewer)
library(tidyr)
library(dplyr)
library(gridExtra)
library(ggfortify)


knitr::opts_knit$set(root.dir = here("0_Materiale"))

```

# Progetto numero 5

NBA moderna (1976-2011): VARIABILE DIPENDENTE: numero di vittorie in stagione COVARIATE: tutte le altre (o uno specifico insieme di queste, in base all'obiettivo di analisi) Considerare solo le squadre che hanno giocato 82 partite (dataset\$games==82)

## INIZIALIZZAZIONE DATI E GRAFICI DATI
```{r}

dataset <- read.delim("basketball_teams.txt") # andiamo a leggere il database fornito

FIRST <- 1976 # primo anno del range da considerare per lo studio
LAST <- 2011 # ultimo anno del range da considerare per lo studio

df <- dataset [dataset$lgID=="NBA" & dataset$year >= FIRST & dataset$year <= LAST & dataset$games==82,]

# dataset$lgID <- as.factor(dataset$lgID) # perchè mi permettono di poter generare variabili dummy

summary(df)

```

```{r}

#STUDIO DATI RELATIVI AI RIMBALZI

# o_oreb = Rimbalzi ottenuti in attacco
# o_dreb = Rimbalzi subiti in attacco
# o_reb  = totale rimbalzi in attacco
# d_oreb = Rimbalzi subiti in difesa
# d_dreb = Rimbalzi ottenuti in difesa
# d_reb  = totale rimbalzi in difesa

df_reb <- subset(df, select = c("o_oreb", "o_dreb", "o_reb", "d_oreb", "d_dreb", "d_reb", "won"))

par(mfrow = c(2, 3))  # Imposta il layout a 2 righe e 3 colonne

for (variables in 1:(dim(df_reb)[2]-1)) {
  thisvar = df_reb[,variables]
  d <- density(thisvar)
  xmin <- floor(min(thisvar))
  xmax <- ceiling(max(thisvar))
  
  # Crea il plot della densità con stile accattivante
  plot(d, main = names(df_reb)[variables], xlab = "", col = "blue", lwd = 1.5, xlim = c(xmin, xmax), ylim = c(0, max(d$y)*1.1))

  # Aggiungi la distribuzione normale teorica ideale in rosso
  x <- seq(xmin, xmax, length = 100)
  lines(x, dnorm(x, mean = mean(thisvar), sd = sd(thisvar)), col = "red", lwd = 1.5)  # Modifica lo spessore delle linee

  # Aggiungi griglia per migliorare la leggibilità
  grid()
}

# Aggiungi titolo in grassetto e corsivo, con spessore del testo modificato
title(bquote(bold(italic("Density plots with Normal Distribution"))), line = -17, cex.main = 2, outer = TRUE)



#verifica della presenza zeri nel dataset
print(paste("Percentage non-zero o_oreb: ",round(length(which(df_reb$o_oreb>0)) / dim(df_reb)[1]*100,2)))
print(paste("Percentage non-zero o_dreb: ",round(length(which(df_reb$o_dreb>0)) / dim(df_reb)[1]*100,2)))
print(paste("Percentage non-zero o_reb: ",round(length(which(df_reb$o_reb>0)) / dim(df_reb)[1]*100,2)))
print(paste("Percentage non-zero d_oreb: ",round(length(which(df_reb$d_oreb>0)) / dim(df_reb)[1]*100,2)))
print(paste("Percentage non-zero d_dreb: ",round(length(which(df_reb$d_dreb>0)) / dim(df_reb)[1]*100,2)))
print(paste("Percentage non-zero d_reb: ",round(length(which(df_reb$d_reb>0)) / dim(df_reb)[1]*100,2)))

```

```{r}
#STUDIO SULLE VITTORIE

#ISTOGRAMMA
# Crea un istogramma di base
hist(df$won, col = "skyblue", border = "white", main = "Distribuzione delle Vittorie", xlab = "Numero di Vittorie", ylab = "Frequenza")

# Aggiungi una griglia di sfondo
grid()

# Aggiungi una linea di riferimento
abline(v = mean(df$won), col = "red", lwd = 2)

# Aggiungi una legenda
legend("topright", legend = c("Media"), col = c("red"), lwd = 2)



#ISTOGRAMMA INTERATTIVO

# Crea un istogramma con plot_ly
histogram <- plot_ly(df, x = ~won, type = "histogram", 
                     marker = list(color = "skyblue", line = list(color = "white", width = 0.5)),
                     opacity = 0.7, name = "Campione") %>%
  layout(title = list(text = "<b><i>Distribuzione delle Vittorie</i></b>",y = 0.97),
         xaxis = list(title = "<b><i>Numero di Vittorie</i></b>", zeroline = FALSE),
         yaxis = list(title = "<b><i>Frequenza</i></b>", zeroline = FALSE),
         barmode = "overlay") %>%
  add_trace(x = ~mean(won), type = "scatter", mode = "lines", 
            line = list(color = "red", width = 2), name = "<i><b>Media</i></b>") %>%
  add_trace(x = ~mean(won), type = "scatter", mode = "markers", 
            marker = list(color = "red", size = 8), showlegend = FALSE) %>%
  add_annotations(text = sprintf("<i><b>Media: %.2f</b></i>", mean(df$won)), x = mean_value, y = 0, 
                  arrowhead = 2, arrowcolor = "red", arrowsize = 1.5, arrowwidth = 2)

# Visualizza l'istogramma interattivo
histogram



# PLOT DENSITA

density_plot <- density(df$won)

# Plot di base
plot(density_plot, main = "Distribuzione di Densità delle Vittorie", col = "skyblue", lwd = 2, ylim = c(0, 0.07), xlim = c(0, max(df$won)))

# Aggiungi titoli ed etichette degli assi
title(main = "Distribuzione di Densità delle Vittorie", xlab = "Numero di Vittorie", ylab = "Densità")

# Aggiungi una griglia di sfondo
grid()

# Aggiungi una linea di riferimento per la media
abline(v = mean(df$won), col = "red", lwd = 2)

# Aggiungi una legenda
legend("topright", legend = c("Media"), col = c("red"), lwd = 2)


# PLOT DENSITA INTERATTIVO

# Calcola la densità delle vittorie
density_plot <- density(df$won)

# Crea il plot di densità con plot_ly
density_interactive <- plot_ly(x = density_plot$x, y = density_plot$y, type = "scatter", mode = "lines", 
                               line = list(color = "skyblue", width = 2), name = "Densità") %>%
  layout(title = list(text = "<b><i>Distribuzione di Densità delle Vittorie</i></b>", x = 0.5),
         xaxis = list(title = "<i>Numero di Vittorie</i>"),
         yaxis = list(title = "<i>Densità</i>"),
         showlegend = TRUE) %>%
  add_trace(x = c(mean(df$won), mean(df$won)), y = c(0, max(density_plot$y)), 
            type = "scatter", mode = "lines", line = list(color = "red", width = 2),
            name = "<i><b>Media</b></i>") %>%
  add_trace(x = mean(df$won), y = max(density_plot$y), type = "scatter", mode = "markers", 
            marker = list(color = "red", size = 8), showlegend = FALSE) %>%
  add_annotations(text = sprintf("<i><b>Media: %.2f</b></i>", mean(df$won)), x = mean(df$won), y = max(density_plot$y) * 1.05, 
                  arrowhead = 2, arrowcolor = "red", arrowsize = 1.5, arrowwidth = 2, 
                  ax = 0, ay = -40)

# Visualizza il plot di densità interattivo
density_interactive



# CORRPLOT

data_subset <- df[, c(11:40, 54)]

cor_matrix <- cor(data_subset, use = "complete.obs")  

cor_data <- melt(cor_matrix)
names(cor_data) <- c("Variable1", "Variable2", "Correlation")

# Create the heatmap
p <- plot_ly(data = cor_data, x = ~Variable1, y = ~Variable2, z = ~Correlation, 
             type = "heatmap", 
             colors = viridis(n = 1024, option = "magma"),
             hoverinfo = "x+y+z") %>% 
  layout(
    title = '<b><i>Correlation Matrix Heatmap</i></b>',
    xaxis = list(
      title = list(text = "<b><i>Variabile 1</i></b>", standoff = 0),
      tickangle = 45,
      zeroline = FALSE
    ),
    yaxis = list(
      title = list(text = "<b><i>Variabile 2</i></b>", standoff = 0),
      tickangle = 45,
      zeroline = FALSE
    ),
    autosize = TRUE
  )

# AGGIUNTA VALORI NELLE CELLE
# cor_values <- round(as.matrix(cor_matrix), 2)  # Round for readability
# for (i in seq_len(nrow(cor_matrix))) {
#   for (j in seq_len(ncol(cor_matrix))) {
#     p <- p %>% add_annotations(
#       x = rownames(cor_matrix)[i],
#       y = colnames(cor_matrix)[j],
#       text = as.character(cor_values[i, j]),
#       showarrow = FALSE,
#       font = list(color = ifelse(cor_values[i, j] < 0.5, "white", "black"))
#     )
#   }
# }

(p)



#BOXPLOT

aggregated_data <- df %>%
  select(tmID, year, won) %>%
  group_by(tmID, year) %>%
  summarize(
    won = mean(won, na.rm = TRUE), 
  )

reshaped_data <- aggregated_data %>%
  pivot_longer(cols = c(won), names_to = "stat_type", values_to = "stat") %>%
  unite("new_col", tmID, stat_type, sep = "_") %>%
  pivot_wider(names_from = new_col, values_from = "stat")

fig <- plot_ly()


for (i in 2:ncol(reshaped_data)) {
    current_column_data <- reshaped_data[[i]]
    fig <- fig %>% add_trace(y = current_column_data, name = colnames(reshaped_data)[i], type = "box")
}

(fig)

df$reb <- df$o_reb + df$d_reb

```

## TESTS DI VERIFICA

### TEST ANDERSON-DARLING

```{r}
ad.test(df$reb)
```

Con un livello di significatività ($\alpha$) di 0.01 e un p-value molto piccolo (3.1e-09) ottenuto dal test di normalità di Anderson-Darling per i dati della variabile df\$reb, puoi concludere che hai sufficiente evidenza statistica per respingere lipotesi nulla che i dati seguono una distribuzione normale.Con il tuo livello di significatività del 0.01 e il p-value molto piccolo (3.1e-09), il p-value è inferiore al livello di significatività, quindi respingeresti lipotesi nulla. Questo suggerisce che i dati nella variabile df\$reb non seguono una distribuzione normale al livello di significatività del 0.01. In termini più pratici, hai abbastanza evidenza statistica per concludere che la variabile df\$reb non segue una distribuzione normale basandoti sui risultati del test di Anderson-Darling.

### TEST KOLMOGOROV SMIRNOV

```{r}
ks.test(df$reb, "pnorm")
```

Il risultato che hai ottenuto riguarda il test di Kolmogorov-Smirnov a campione singolo sui dati contenuti nella variabile df\$reb. Il test KS confronta la distribuzione empirica dei tuoi dati con una distribuzione teorica (spesso una distribuzione uniforme). In breve, il risultato suggerisce che i tuoi dati non seguono la distribuzione teorica presunta, e cè un elevata probabilità che la differenza osservata sia statisticamente significativa.

### TEST SHAPIRO WILK

```{r}
sf.test(df$reb)
```

In sintesi, il risultato del test di Shapiro-Francia indica che i tuoi dati nella variabile df\$reb non seguono una distribuzione normale. Questo è supportato dal valore basso del p-value, il quale suggerisce che la differenza tra la distribuzione dei tuoi dati e una distribuzione normale è statisticamente significativa.

## INIZIALIZZAZIONE MODELLO DI REGRESSIONE LINEARE

### L'IMPORTANZA DEI RIMBALZI

$\text{Formula1} = \frac{\text{Rimbalzi offensivi in attacco}}{\text{Tiri sbagliati su azione}}$ Rappresenta la capacità della squadra di ripossesso della palla dopo un tiro che non va a canestro e colpisce il tabellone.

$\text{Formula2} = \frac{\text{Rimbalzi difensivi in difesa presi}}{\text{Tiri sbagliati su azione degli avversari}}$ Rappresenta la capacità della squadra di impossessarsi della palla dopo un tiro sbagliato della squadra avversaria che colpisce il tabellone, che troviamo un buon stimatore della capacità di contropiede della squadra.

$\text{Formula3} = \frac{\text{Palle riprese in attacco} + 1.5 \times \text{Palle riprese in difesa}}{\text{Palle perse in attacco} + 2 \times \text{Rimbalzi subiti in difesa}}$ Rappresenta il rapporto tra le palle riprese nei rimbalzi (sia offensivi che difensivi) rispetto alle palle perse nei rimbalzi (sia offensivi che difensivi). I coefficienti sono stati scelti in base a ciò che riteniamo più importante in una partita, ossia la difesa del proprio canestro.

$\text{Formula4} = (\text{Palle riprese in attacco - Palle perse in attacco}) + 1.5*(\text{Palle riprese in difesa - Palle perse in difesa})$ Cresce all'aumentare dei rimbalzi ottenuti e diminuisce all'aumentare dei rimbalzi subiti, considerando anche un coefficiente che da particolare importanza alla difesa.

$\text{Formula5} = \frac{(\frac{\text{Rimbalzi subiti in difesa}}{\text{Palle perse in difesa}})}{(\frac{\text{Rimbalzi subiti in attacco}}{\text{Palle perse in attacco}})}$ Mostra quanto siano influenti i rimbalzi nel rapporto tra le palle perse dalla squadra e le palle perse dagli avversari.

```{r}
# o_oreb = Rimbalzi ottenuti in attacco
# o_dreb = Rimbalzi subiti in attacco
# o_reb  = totale rimbalzi in attacco
# d_oreb = Rimbalzi subiti in difesa
# d_dreb = Rimbalzi ottenuti in difesa
# d_reb  = totale rimbalzi in difesa

df$f1 <- (df$o_oreb)/(df$o_fga-df$o_fgm)
df$f2 <- (df$d_dreb)/(df$d_fga-df$d_fgm)
df$f3 <- (df$o_oreb + 1.5 * df$d_dreb)/(df$o_dreb + 2 * df$d_oreb)
df$f4 <- (df$o_oreb - df$o_dreb) + 1.5 * (df$d_dreb - df$d_oreb)
df$f5 <- (df$d_oreb / df$d_to) / (df$o_dreb / df$o_to)


linMod <- lm(won ~ f1 + f2 + f3 + f4 + f5, data = df)
summary (linMod)

# Imposta il layout della pagina
par(mfrow = c(2, 2))

# Crea i quattro grafici
plot(linMod)

# Ripristina il layout predefinito
par(mfrow = c(1, 1))


```

## INIZIALIZZAZIONE MODELLO DI REGRESSIONE LINEARE NORMALIZZATO

```{r}
# In un chunk diverso per minimizzare cpu-time

# Normalizziamo le covariate
df$f1_z <- scale(df$f1)
df$f2_z <- scale(df$f2)
df$f3_z <- scale(df$f3)
df$f4_z <- scale(df$f4)
df$f5_z <- scale(df$f5)

linModNormalized <- lm(won ~ f1_z + f2_z + f3_z + f4_z + f5_z, data = df)

summary(linModNormalized)

plot(linModNormalized)

# Imposta il layout della pagina
par(mfrow = c(2, 2))

# Crea i quattro grafici
plot(linMod)

# Ripristina il layout predefinito
par(mfrow = c(1, 1))

```

## TEST SUL MODELLO DI REGRESSIONE LINEARE

### TEST BREUSCH-PAGAN (Test di omoschedasticità)

```{r}

# TEST SUL MODELLO DI REGRESSIONE LINEARE

#1 Summary
summary (linModNormalized)

#2 R-quadrato e R-quadrato Adattato
summary_linModNormalized <- summary(linModNormalized)
r_squared <- summary_linModNormalized$r.squared
cat("R-squared:", r_squared, "\n")


n <- length(df$o_oreb)
k <- length(linModNormalized$coefficients) - 1
adjusted_r_squared <- 1 - ((1 - r_squared) * (n - 1) / (n - k - 1))
cat("Adjusted R-squared:", adjusted_r_squared, "\n")

#2 test Shapiro per valutare la normalita' dei residui
shapiro.test(residuals(linModNormalized))

#3 test di omoschedasticita' (Breusch-Pagan test) --> risultato suggerisce omoschedasiticita'
bptest(linModNormalized)

#4 test di multicollinearita'
car::vif(linModNormalized)

```

Possiamo ora verificare la presenza di outlayers, ovvero dati anomali o fuori norma che possono influenzare in modo molto significativo il modello rispetto al resto dei dati presi in cosiderazione.
Provvediamo quindi a rimuoverli e a ricreare il modello.
```{r, include=TRUE}

influencePlot(linModNormalized,5)

#rimuovo gli outlayers e ricreo il modello
df_1 <- df[-which(row.names(df) %in% c(822, 957)),]

df_1$f1 <- (df_1$o_oreb)/(df_1$o_fga-df_1$o_fgm)
df_1$f2 <- (df_1$d_dreb)/(df_1$d_fga-df_1$d_fgm)
df_1$f3 <- (df_1$o_oreb + 1.5 * df_1$d_dreb)/(df_1$o_dreb + 2 * df_1$d_oreb)
df_1$f4 <- (df_1$o_oreb - df_1$o_dreb) + 1.5 * (df_1$d_dreb - df_1$d_oreb)
df_1$f5 <- (df_1$d_oreb / df_1$d_to) / (df_1$o_dreb / df_1$o_to)

df_1$f1_z <- scale(df_1$f1)
df_1$f2_z <- scale(df_1$f2)
df_1$f3_z <- scale(df_1$f3)
df_1$f4_z <- scale(df_1$f4)
df_1$f5_z <- scale(df_1$f5)

linModNormalized_1 <- lm(won ~ f1_z + f2_z + f3_z + f4_z + f5_z, data = df_1)

influencePlot(linModNormalized_1,5)

summary(linModNormalized)
summary(linModNormalized_1)


```
Nel complesso entrambi i modelli sembrano essere buoni, spiegano entrambi circa l'84% della varianza nella variabile dipendente "won", anche se la rimozione degli otlayers ha migliorato leggermente i risultati.

Dato che la differenza tra i due modelli e' molto ridotta (<0.5%), tale da essere trascurabile, abbiamo deciso di utilizzare il modello comprendente gli outlayers.


```{r}
# Divisione in Test e Train per evitare che il modello fitti troppo bene sui nostri dati
sample <- sample(c(TRUE, FALSE), nrow(df), replace=TRUE, prob=c(0.7, 0.3))
train  <- df_reb[sample, ]
test   <- df_reb[!sample, ]

```


```{r}
# METODO DI REGRESSIONE LASSO

#define response variable
y <- train$won

#define matrix of predictor variables (uso solo poche variabili ma potete farlo con tutte da togliere però la risposta area)
x <- data.matrix(train)

#perform k-fold cross-validation to find optimal lambda value, la cross-validation è un ottimo modo per non overfittare e trovare il miglior modello
cv_model <- cv.glmnet(x, y, alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda

# Addestramento del modello con cross-validation
cv_model <- cv.glmnet(x, y)

# Grafico personalizzato
plot(cv_model, xvar="lambda", main="", xlab="log(Lambda)", ylab="Mean Squared Error", col="blue", lwd=2)

# Aggiungi un titolo personalizzato con formattazione LaTeX per grassetto e corsivo
title(main=expression(bold(italic("Validazione Incrociata"))), line = 3)


# Fittiamo il modello con il best lambda (penalizzazione)
best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(best_model)


```



```{r}

# SANDBOX TESTING
# QUI SI TESTA TUTTO CIÒ CHE POI VERRÀ AGGGIUNTO NEL DOCUMENTO SOPRA

values <- aggregate(cbind(o_oreb, o_dreb, d_oreb, d_dreb, o_reb, d_reb, won) ~ tmID, data = df, FUN = sum)

# temp <- hist (temp, col = 'steelblue', main = 'caccaculo', xlab = 'balls')

### SANDBOX

teams <- c()

df$reb <- c(df$o_reb + df$d_reb)

summary(df$reb)

for (team in df$tmID)
{
  teams <- unique(c(teams, team))
}

teams <- sort(teams)

values <- aggregate(cbind(reb, o_oreb, o_dreb, d_oreb, d_dreb, o_reb, d_reb, won) ~ tmID, data = df, FUN = sum)

x <- seq(-4, 4, by = 0.01)
y <- dnorm(x, mean = 0, sd = 1)
plot (x,y, type="l")
normal <- data.frame(x, y)


# ISTOGRAMMA INTERATTIVO


plot_ly(data = df, x = ~reb, type = "histogram",
        marker = list(color = 'skyblue', line = list(color = 'black', width = 1)),
        nbinsx = 20, legendgroup = "Rimbalzi", name = "Campione") %>% 
  add_trace(type = "scatter", mode = "lines",
            x = c(mean(df$reb), mean(df$reb)),
            y = c(0, 215),
            line = list(color = "red", width = 2, dash = "dash"),
            name = "Media") %>%
  layout(title = list(text = "<b><i>Istogramma dei Rimbalzi</i></b>", pad = 10),
         xaxis = list(title = '<b><i>Rimbalzi</i></b>'),
         yaxis = list(title = '<b><i>Frequenza</i></b>'),
         legend = list(title = "<b><i>Legenda</i></b>", tracegrouporder = "reversed"))



#PLOT DI DENSITA CON SOVRAPPOSIZIONE DI UNA NORMALE IDEALE

density_data <- density(df$reb)
mu <- mean(df$reb)
sigma <- sd(df$reb)
normal_data <- dnorm(density_data$x, mean = mu, sd = sigma)

p <- plot_ly(x = density_data$x, y = density_data$y, type = 'scatter', mode = 'lines',
             line = list(color = 'blue', width = 2),
             name = "Densità rimbalzi totale") %>%
  layout(title = "Densità dei Rimbalzi",
         xaxis = list(title = "Rimbalzi"),
         yaxis = list(title = "Density", autotick = TRUE, autorange = TRUE))

p <- add_trace(p, x = density_data$x, y = normal_data, mode = 'lines',
               line = list(color = 'green', width = 2),
               fill = "tozeroy", fillcolor = "rgba(0, 255, 0, 0.2)",
               name = "Dist normale ideale")

p <- add_trace(p, x = c(mu, mu), y = c(0, max(density_data$y)),
               mode = 'lines', line = list(color = 'red', width = 2, dash = 'dash'),
               name = "Media")

(p)

# CORRPLOT

data_subset <- df[, c("reb", "o_reb", "d_reb")]
cor_matrix <- cor(data_subset)

rownames(cor_matrix) <- colnames(data_subset)
colnames(cor_matrix) <- colnames(data_subset)

cor_data <- reshape2::melt(cor_matrix)
names(cor_data) <- c("Var1", "Var2", "Corr")

dimnames(cor_matrix) <- list(rownames(cor_matrix), colnames(cor_matrix))
data_for_plotly <- as.data.frame(as.table(cor_matrix))

p <- plot_ly(data = cor_data, 
             x = ~Var1, 
             y = ~Var2, 
             z = ~Corr, 
             type = "heatmap", 
             colors = colorRampPalette(c("#4575b4", "#91bfdb", "#e0f3f8", "#fee08b", "#d73027"))(100),
             hoverinfo = "x+y+z") %>% 
  layout(title = 'Correlation Matrix',
         xaxis = list(title = "", tickangle = 45, side = "bottom", automargin = TRUE),
         yaxis = list(title = "", automargin = TRUE),
         autosize = TRUE)

cor_values <- round(as.matrix(cor_matrix), 2)  # Round for readability
for (i in seq_len(nrow(cor_matrix))) {
  for (j in seq_len(ncol(cor_matrix))) {
    p <- p %>% add_annotations(
      x = rownames(cor_matrix)[i],
      y = colnames(cor_matrix)[j],
      text = as.character(cor_values[i, j]),
      showarrow = FALSE,
      font = list(color = ifelse(cor_values[i, j] < 0.5, "white", "black"))
    )
  }
}

(p)


#BOXPLOT (1° Versione)

aggregated_data <- df %>%
  select(tmID, year, o_reb, d_reb) %>%
  group_by(tmID, year) %>%
  summarize(
    o_reb = mean(o_reb, na.rm = TRUE), 
    d_reb = mean(d_reb, na.rm = TRUE),
    .groups = "drop"  # Aggiunto per evitare il raggruppamento
  )

reshaped_data <- aggregated_data %>%
  pivot_longer(cols = c(o_reb, d_reb), names_to = "stat_type", values_to = "stat") %>%
  unite("new_col", tmID, stat_type, sep = "_") %>%
  pivot_wider(names_from = new_col, values_from = "stat")

fig <- plot_ly()

for (i in 2:ncol(reshaped_data)) {
    current_column_data <- reshaped_data[[i]]
    fig <- fig %>% add_trace(y = current_column_data, name = colnames(reshaped_data)[i], type = "box")
}

(fig)


```



#TEST ANDERSON-DARLING
```{r}
ad.test(df$reb)
```
#Con un livello di significatività (α) di 0.01 e un p-value molto piccolo (3.1e-09) ottenuto dal test di normalità di Anderson-Darling per i dati della variabile df$reb, puoi concludere che hai sufficiente evidenza statistica per respingere lipotesi nulla che i dati seguono una distribuzione normale.Con il tuo livello di significatività del 0.01 e il p-value molto piccolo (3.1e-09), il p-value è inferiore al livello di significatività, quindi respingeresti lipotesi nulla. Questo suggerisce che i dati nella variabile df$reb non seguono una distribuzione normale al livello di significatività del 0.01. In termini più pratici, hai abbastanza evidenza statistica per concludere che la variabile df$reb non segue una distribuzione normale basandoti sui risultati del test di Anderson-Darling.


#TEST KOLMOGOROV SMIRNOV
```{r}
ks.test(df$reb, "pnorm")
```
# Il risultato che hai ottenuto riguarda il test di Kolmogorov-Smirnov a campione singolo sui dati contenuti nella variabile df$reb. Il test KS confronta la distribuzione empirica dei tuoi dati con una distribuzione teorica (spesso una distribuzione uniforme). In breve, il risultato suggerisce che i tuoi dati non seguono la distribuzione teorica presunta, e cè un elevata probabilità che la differenza osservata sia statisticamente significativa.


#TEST SHAPIRO WILK
```{r}
sf.test(df$reb)
```
#In sintesi, il risultato del test di Shapiro-Francia indica che i tuoi dati nella variabile df$reb non seguono una distribuzione normale. Questo è supportato dal valore basso del p-value, il quale suggerisce che la differenza tra la distribuzione dei tuoi dati e una distribuzione normale è statisticamente significativa.

```{r}
barplot(df$reb, col = c("#1b98e0", "#353436"))

```

```{r}

heatmap_df <- subset(values, select = -c(o_oreb, o_dreb, d_oreb, d_dreb, won, reb))
rownames(heatmap_df) <- heatmap_df$tmID
heatmap_df <- heatmap_df[,-1]

heatmaply(
 heatmap_df,
 colors = viridis(n = 256,  option = "magma"),
 k_col = 2,
 k_row = 4,
)

plot_ly(values, x = ~tmID, y = ~o_reb, type = 'bar', name = 'Rimbalzi offensivi', marker = list(color = '#FFAFA1')) %>%
  add_trace(y = ~d_reb, name = 'Rimbalzi difensivi', marker = list(color = '#b2fff8')) %>%
  layout(yaxis = list(title = 'Valori'), barmode = 'stack')


```

#VARIABILI DUMMY

```{r}

# VARIABILI DUMMY CON TEST ANOVA su ConferenceID

# Controlla i valori univoci nella colonna 'conference'
unique_conferences <- unique(df$confID)
print(unique_conferences)

# Supponiamo che 'conference' sia la variabile categorica di interesse
df$conference <- as.factor(df$confID)

# Creazione di variabili dummy
dummy_vars <- model.matrix(~ df$confID - 1, data=df)  # -1 per evitare la colinearità

# Aggiunta delle variabili dummy al dataframe
df <- cbind(df, dummy_vars)

# Esecuzione di un test ANOVA
# Supponiamo che 'won' sia la variabile dipendente
anova(lm(df$won ~ df$confID, data=df))

# si tiene l'ipotesi nulla perchè la variabile correlazione non ha un effetto sulle vittorie. 
# Non Posso inserire a modello la variabile




# VARIABILI DUMMY CON TEST ANOVA su DivisionID

# Controlla i valori univoci nella colonna 'conference'
unique_conferences <- unique(df$divID)
print(unique_conferences)

# Supponiamo che 'conference' sia la variabile categorica di interesse
df$conference <- as.factor(df$divID)

# Creazione di variabili dummy
dummy_vars <- model.matrix(~ df$divID - 1, data=df)  # -1 per evitare la colinearità

# Aggiunta delle variabili dummy al dataframe
df <- cbind(df, dummy_vars)

# Esecuzione di un test ANOVA
# Supponiamo che 'won' sia la variabile dipendente
anova(lm(df$won ~ df$divID, data=df))


# si rifiuta l'ipotesi perchè la variabile divisione ha un effetto sulle vittorie. Posso inserire a modello la variabile

linModNormalized1 <- lm(won ~ f1_z + f2_z + f3_z + f4_z + f5_z + divID, data = df)
summary(linModNormalized1)

```

#EFFETTI INTERAZIONE

```{r}

linModNormalized1 <- lm(won ~ f1_z + f2_z + f3_z + f4_z + f5_z + divID + f1_z:f2_z + f4_z:divID, data = df)
summary(linModNormalized1)


# f4 è indicatore che cresce all'aumentare dei rimbalzi ottenuti e diminuisce all'aumentare dei rimbalzi subiti.
# Interazione tra f4 e la divisione non è significativa, contrariamente al fatto che invece interazione tra f1 e f2 è significativo


# AGGIORNAMENTO Del modello lineare togliendo la seconda interazione

linModNormalized2 <- lm(won ~ f1_z + f2_z + f3_z + f4_z + f5_z + divID + f1_z:f2_z, data = df)


```


#test F, significatività singoli coefficienti, R^2

```{r}

summary(linModNormalized2)

#Nel complesso, il modello sembra ben adattato ai dati, spiegando una parte significativa della variazione nelle vittorie delle squadre di pallacanestro. La significatività delle variabili derivate (f1_z, f2_z, f3_z, f4_z, f5_z) e delle variabili dummy divID suggerisce che questi fattori sono importanti nella predizione del numero di vittorie. Il termine di interazione f1_z:f2_z è significativo, indicando che c'è un effetto sinergico tra le variabili f1_z e f2_z nella loro influenza sul numero di vittorie.

```

# Può aiutare l’uso della POISSON?

```{r}

linModNormalized2_pois = glm(won ~ f1_z + f2_z + f3_z + f4_z + f5_z + divID + f1_z:f2_z, family=poisson(link=log), data = df)
summary(linModNormalized2_pois)


normal = coefficients(linModNormalized2)
poisson = exp(coefficients(linModNormalized2_pois))

cbind(normal, poisson)

#Considerazioni Generali:
#È importante notare che gli effetti delle variabili possono essere interpretati in modo diverso a seconda della distribuzione scelta per il modello. La scelta tra distribuzione normale e di Poisson dipende dalla natura della tua variabile dipendente e dai tuoi obiettivi di modellazione. Nel contesto di modelli di regressione, è sempre buona pratica verificare l'adeguatezza del modello esaminando i residui, eseguendo test diagnostici e valutando la bontà di adattamento. L'interpretazione dei coefficienti dovrebbe essere fatta considerando la scala appropriata per la distribuzione utilizzata (lineare per la normale, logaritmica per la Poisson). Se stai cercando di prevedere il numero di vittorie, la distribuzione di Poisson potrebbe essere più appropriata per variabili conteggio come questa. Tuttavia, è sempre necessario verificare l'adeguatezza del modello ai dati specifici.



```